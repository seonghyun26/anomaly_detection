{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "927e3e35",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 1. Generator-based anomaly detection (basic)\n",
    "### 1.1. Gaussian model\n",
    "### 1.2. Kernel density estimation\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "## 2. Autoencoder-based anomaly detection\n",
    "### 2.1. Using reconstruction error\n",
    "### 2.2. Using pre-trained autoencoder with generator-based approach\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "## 3. Likelihood ratio\n",
    "### 3.1. Likelihood ratio with Gaussian model (basic)\n",
    "### 3.2. Likelihood ratio with Autoregressive model (Advanced)\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abedafe6",
   "metadata": {},
   "source": [
    "# 1. Generator-based anomaly detection\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 1.1. Overview\n",
    "\n",
    "**Data distribution $p(x)$를 학습하여, 새로 주어진 target data가 outlier인지 판단함.**\n",
    "\n",
    "1. **Data distribution** $p(x)$를 표현하는 **모델**을 학습함. \n",
    "\n",
    "2. 만약, 새로 주어진 **target data** $x'$이 **data distribution**에 속할 확률이 낮다면 ($p(x')<\\epsilon$), outlier로 판단함.\n",
    "\n",
    "<img src=\"./images/generator_example.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 1.2. Method\n",
    "\n",
    "### 1.2.1. Gaussian model\n",
    "\n",
    "**Data distribution $p(x)$를 다루기 쉬운 Gaussian distribution으로 가정하는 프레임워크.**\n",
    "\n",
    "1. 주어진 데이터셋에 대한 분포를 표현할 수 있는 **Gaussian distribution**를 추정함. \n",
    "\n",
    "2. 만약 주어진 **target data**가 **추정된 Gaussian distribution**에서 얻어질 확률이 작다면 ($p(x')<\\epsilon$), 해당 데이터를 **outlier**로 판단함.\n",
    "\n",
    "<img src=\"./images/gaussian_example.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### 1.2.2. Kernel density estimation (KDE)\n",
    "\n",
    "**Data distribution $p(x)$를 모든 training data point에 대한 point-wise kernel을 이용하여 표현함.** \n",
    "\n",
    "1. 모든 training data로 표현된 **kernel density**로부터 **target data**가 얻어질 확률을 값을 계산함.\n",
    "\n",
    "2. 계산된 확률 값이 일정 이하라면, 해당 데이터를 **outlier**로 판단함.\n",
    "\n",
    "<img src=\"./images/kde_example.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2371dc1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 1.3. Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73c7956c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Install / Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library \n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install --upgrade setuptools\n",
    "!pip3 install numpy\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acded75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Define toy dataset (2-dimensional data)\n",
    "\n",
    "- 간단한 예시로서, **2-dimensional toy dataset**에서 anomaly detection을 수행함. \n",
    "\n",
    "- **training data는 총 100개**이며, **test data는 20개** (10개는 anomaly, 남은 10개는 normal data).\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Variable list:**\n",
    "\n",
    "> **N**: training data의 개수 (=100).\n",
    "\n",
    "> **M**: test data의 개수 (=20).\n",
    "\n",
    "> **training_data**: training dataset에 대한 numpy matrix ($N \\times 2$).\n",
    "\n",
    "> **test_data**: test dataset에 대한 numpy matrix ($M \\times 2$).\n",
    "\n",
    "> **test_data_label**: 각각의 test data가 anomaly data인가에 대한 ground truth (boolean array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # Number of training data\n",
    "M = 20 # Number of test data\n",
    "\n",
    "\n",
    "# Generate some normal data points\n",
    "np.random.seed(0)\n",
    "mean_normal = np.array([5, 5])\n",
    "cov_normal = np.array([[1, 0.5], [0.5, 1]])\n",
    "training_data = np.random.multivariate_normal(mean_normal, cov_normal, N) # normal data\n",
    "\n",
    "\n",
    "# Generate some test data points\n",
    "mean_anomaly = np.array([7, 7])\n",
    "cov_anomaly = np.array([[2, -1], [-1, 2]])\n",
    "test_data_normal = np.random.multivariate_normal(mean_normal, cov_normal, M//2) # normal data\n",
    "test_data_anomaly = np.random.multivariate_normal(mean_anomaly, cov_anomaly, M//2) # anomaly data\n",
    "test_data = np.concatenate([test_data_normal,test_data_anomaly], axis = 0) # concatenate\n",
    "test_data_label = np.array([False]*(M//2)+[True]*(M//2)) # True indicates the anomaly data point\n",
    "\n",
    "print('training_data.shape:', training_data.shape)\n",
    "print('target_data.shape:', test_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot training data in green\n",
    "plt.scatter(training_data[:, 0], training_data[:, 1], s = 20,c='green', label='Training Data')\n",
    "# Plot test data\n",
    "plt.scatter(test_data[:, 0], test_data[:, 1], c=[('red' if v else 'blue') for v in test_data_label])\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b89538f",
   "metadata": {},
   "source": [
    "- **우리는 위의 데이터에서 anomaly detection을 예시로서 수행함.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "### Anomaly detection with Gaussian model [1/3]\n",
    "\n",
    "**우선, data distribution을 Gaussian distribution $p(x;\\mu,\\Sigma)$으로 표현함.** 구체적으로,\n",
    "\n",
    "- **Gaussian distribution** $p(x;\\mu,\\Sigma)$를 표현하는 파라미터인 **mean**과 **covariance** $(\\mu,\\Sigma)$를 **training data**로 계산해야함.\n",
    "\n",
    "- **Mean**과 **covariance** $(\\mu,\\Sigma)$는 다음과 같이 계산 가능함.\n",
    "$$\\mu=\\frac{1}{N}\\sum_{n=1}^N x_n, \\qquad \\Sigma=\\frac{1}{N}\\sum_{n=1}^N (x_n-\\mu)(x_n-\\mu)^T$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba10fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_Gaussian_parameter(data):\n",
    "    \"\"\"Obtain parameters of Gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: N x 2 array (training data, where row indicies each data point)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean: single float\n",
    "    covariance: single float\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" You can use functions in libraries.\n",
    "    mean: https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "    covariance: https://numpy.org/doc/stable/reference/generated/numpy.cov.html\n",
    "    \"\"\" \n",
    "\n",
    "    number_of_data = data.shape[0]\n",
    "\n",
    "    # Compute mean and covariance using numpy module\n",
    "    mean = \"\"\" TODO \"\"\"\n",
    "    cov = \"\"\" TODO \"\"\"\n",
    "\n",
    "    # (Optional) Compute mean parameter directly\n",
    "    mean = 0\n",
    "    for i in range(\"\"\" TODO \"\"\"):\n",
    "        mean = \"\"\" TODO \"\"\"\n",
    "    mean = mean / \"\"\" TODO \"\"\"\n",
    "\n",
    "    # (Optional) Compute covariance parameter directly\n",
    "    cov = np.zeros((2, 2))\n",
    "    for i in range(\"\"\" TODO \"\"\"):\n",
    "        de_mean = \"\"\" TODO \"\"\" # x_i - mean\n",
    "        de_mean = de_mean.reshape(2,1) # Convert 2-dimensional vector to (2x1) matrix\n",
    "        cov = cov + np.dot(\"\"\" TODO \"\"\",\"\"\" TODO \"\"\".T) # (x_i - mean) (x_i - mean).T\n",
    "    cov = cov / \"\"\" TODO \"\"\"\n",
    "\n",
    "    return mean, cov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Anomaly detection with Gaussian model [2/3]\n",
    "\n",
    "- **Gaussian distribution**의 파라미터 ($\\mu,\\Sigma$)가 주어졌을 때, 테스트 데이터 $x'$의 **likelihood**는 다음과 같음. $$p(x';\\mu,\\Sigma)=\\frac{1}{\\sqrt{(2\\pi)^{2}\\det(\\Sigma)}}\\exp(-\\frac{1}{2}(x'-\\mu)^T\\Sigma^{-1}(x'-\\mu))$$\n",
    "\n",
    "- 계산된 **likelihood** $p(x';\\mu,\\Sigma)$로 데이터 $x'$가 **outlier**인지 판단할 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca118f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood_Gaussian(target, mean, covariance):    \n",
    "    \"\"\"Compute the likelihood of target given Gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target: 2-dimensional array (single target data)\n",
    "    mean: \n",
    "    covariance:\n",
    "    threshhold:\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_anomaly: Bool\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" You can use function in package.\n",
    "    multivariate_normal.pdf: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html\n",
    "    (recomendation) anomaly_score = multivariate_normal.pdf(# TODO #)\n",
    "    \"\"\" \n",
    "    \n",
    "    # Compute likelihood using scipy module given mean and covariance\n",
    "    likelihood = \"\"\" TODO \"\"\"\n",
    "\n",
    "    # (Optional) Compute likelihood directly given mean and covariance\n",
    "    det_cov = np.linalg.det(\"\"\" TODO \"\"\") # determinant\n",
    "    inv_cov = np.linalg.inv(\"\"\" TODO \"\"\") # inverse matrix\n",
    "    exponent = -0.5 * np.dot(np.dot((\"\"\" TODO \"\"\").T, inv_cov), (\"\"\" TODO \"\"\"))\n",
    "    coeff = 1 / (np.sqrt((2 * np.pi) ** 2 * det_cov))\n",
    "    likelihood = coeff * np.exp(exponent)\n",
    "\n",
    "    return likelihood   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a41254ae",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Anomaly detection with Gaussian model [3/3]\n",
    "\n",
    "1. training data distribution을 표현하는 **Gaussian model의 파라미터**를 얻음.\n",
    "\n",
    "2. **test data에 대한 likelihood**를 위의 모델로부터 계산함\n",
    "\n",
    "3. 만약, 주어진 데이터의 **likelihood**가 threshhold $\\epsilon$보다 작다면 $(p(x')<\\epsilon )$, outlier로 판단함.\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **anomaly_predict**: 각각의 test data가 anomaly data인가에 대한 prediction (boolean list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca118f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain mean and covariance parameter through training data\n",
    "mean, cov = \"\"\" TODO \"\"\"\n",
    "threshhold = 0.03\n",
    "\n",
    "# Predict the label of test data (is anomaly?)\n",
    "anomaly_predict = []\n",
    "\n",
    "for i in range(0,M):\n",
    "    likelihood = \"\"\" TODO \"\"\" # Compute the likelihood of test data point\n",
    "\n",
    "    is_anomaly = likelihood < threshhold  # If likelihood is smaller than threshhold, is_anomaly should be True\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediciton:', anomaly_predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb28d3b9",
   "metadata": {},
   "source": [
    "- 예측한 결과에 대한 performance를 F1 score 및 accuracy를 이용하여 측정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "\"\"\" You can use functions in a package.\n",
    "sklearn.metrics.accuracy: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "sklearn.metrics.F1: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\"\"\"\n",
    "\n",
    "accuracy = \"\"\" TODO \"\"\"\n",
    "F1 = \"\"\" TODO \"\"\"\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(1, 10, 100), np.linspace(0, 10.5, 100))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = multivariate_normal.pdf(points, mean=mean, cov=cov)\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "markers = np.where(anomaly_predict==test_data_label, \"o\", \"x\").tolist()\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), marker=markers[i])\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### Anomaly detection with Kernel Density Estimation [1/2]\n",
    "\n",
    "**Data distribution을 각각의 training data에 대한 point-wise kernel을 이용하여 표현함.**\n",
    "\n",
    "- Data point $x'$에 대하여 **kernel density estimation**의 **likelihood**는 다음과 같이 정의됨. $$p(x')=\\frac{1}{N}\\sum^{N}_{i} k(x_i,x'),$$ $\\{x_1,\\ldots, x_N\\}$은 training data이며 $k(\\cdot,\\cdot)$는 kernel. \n",
    "\n",
    "- 각각의 kernel $k(x_i,x')$은 주어진 training data $x_i$를 mean으로 사용하는 Gaussian distribution을 기반으로 정의함. $$k(x_i,x')=\\frac{1}{(2\\pi \\ell^2)}\\exp(-\\frac{1}{2 \\ell^2}\\|x-x_i \\|^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ef2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_density_estimation(dataset, target, bandwidth = 1, threshhold=0.1):\n",
    "    \"\"\"Compute the likelihood of the target using training data based on kernel density estimation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: N x 2 array (training data)\n",
    "    target: 2-dimensional array array (single target data)\n",
    "    bandwidth: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    likelihood: float\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def kernel(data, target, bandwidth):\n",
    "        \"\"\"Compute the kernel value of the target.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: 2-dimensioanl array (single training data)\n",
    "        target: 2-dimensional array (single target data)\n",
    "        bandwidth: float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        k: float\n",
    "        \"\"\"\n",
    "\n",
    "        frac = 1/(2*np.pi*bandwidth)\n",
    "        norm_sq = \"\"\" TODO \"\"\" # ||x - x_i||^2\n",
    "        k = frac * \"\"\" TODO \"\"\" # hint: Use np.exp()\n",
    "        return k\n",
    "\n",
    "\n",
    "    likelihood = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        k = \"\"\" TODO \"\"\"\n",
    "        likelihood = \"\"\" TODO \"\"\"\n",
    "\n",
    "    likelihood = likelihood / N\n",
    "\n",
    "    return likelihood\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Anomaly detection with Kernel Density Estimation [2/2]\n",
    "\n",
    "1. **test data에 대한 likelihood**를 **kernel density estimation**으로부터 계산함\n",
    "\n",
    "2. 만약, 주어진 데이터의 **likelihood**가 threshhold $\\epsilon$보다 작다면 $(p(x')<\\epsilon )$, outlier로 판단."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ef2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = [] \n",
    "threshhold = 0.03\n",
    "\n",
    "for i in range(0,M):\n",
    "    likelihood = \"\"\" TODO \"\"\" # Compute the likelihood of test data point\n",
    "    \n",
    "    is_anomaly = likelihood < threshhold # If likelihood is smaller than threshhold, is_anomaly should be True\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65614a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(1, 10, 30), np.linspace(0, 10.5, 30))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = np.array([kernel_density_estimation(training_data, p) for p in points])\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "markers = np.where(anomaly_predict==test_data_label, \"o\", \"x\").tolist()\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), marker=markers[i])\n",
    "\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "# 2. Anomaly detection with Autoencoder\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 2.1. Overview \n",
    "\n",
    "**Problem: Gaussian model 및 KDE와 같은 방법은 이미지 (e.g., 28 x 28 = 784 dimensional data)와 같은 **high-dimensional data**에 적용하기 어려움.**\n",
    "\n",
    "- **High-dimensional image**를 **low dimension vector** (e.g., 2-dimensional vector)로 mapping하는 **Autoencoder**를 이용하여 문제를 해결함.\n",
    "\n",
    "- **Autoencoder**는 **neural network**로 구성된 **encoder**와 **decoder**로, 데이터를 **압축 및 복원**함.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2. Method\n",
    "\n",
    "### 2.2.1. Reconstruction error-based anomaly detection.\n",
    "\n",
    "**비교적 간단한 방법. test data가 autoencoder에 얼마나 잘 일반화 되었는가를 이용함.**\n",
    "\n",
    "1. **Autoencoder**를 **training data**에 대하여 **reconstruction error** (복원 오차)를 최소화 하도록 학습시킴.\n",
    "\n",
    "2. 만약, 새로 주어진 테스트 데이터 $x'$에 대하여 **reconstruction error**가 일정 이상이면, outlier로 판단함. \n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"./images/autoencoder_1.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### 2.2.2. Pre-trained autoencoder-based anomaly detection.\n",
    "\n",
    "\n",
    "\n",
    "1. 데이터에 대한 **pre-trained auto encoder**를 가지고 있다면, **high-dimensional image**를 **low dimension vector** (e.g., 2-dimensional vector) 로 mapping할 수 있음. \n",
    "\n",
    "3. 그러면, 이미지들이 mapping된 **low dimensional space**에서 **Gaussian model** 및 **KDE**를 적용하여 쉽게 **anomaly detection**을 수행할 수 있음.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"./images/autoencoder.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2371dc1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 2.3. Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73c7956c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Install / Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acded75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Import dataset\n",
    "\n",
    "- **training image는 1000개**이며, **test image는 100개** (50개는 normal data, 나머지 50개는 anomaly data). \n",
    "\n",
    "- 각각의 image는 $28\\times 28=784$ **pixels**를 가졌음.\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **N**: training data의 개수 (=1000).\n",
    "\n",
    "> **M**: test data의 개수 (=100).\n",
    "\n",
    "> **training_data:** 숫자 6에 대한 손글씨 데이터 ($N \\times 784$)\n",
    "\n",
    "> **test_image:** 숫자 6에 대한 손글씨 데이터 ($(M/2) \\times 784$)와 이외의 숫자에 대한 손글씨 데이터 ($(M/2) \\times 784$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a149f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "N = 1000\n",
    "M = 100\n",
    "\n",
    "training_data = torch.cat([data for (data, label) in mnist if label == 6],dim = 0)[:N].view(-1,28*28)\n",
    "test_data_normal = torch.cat([data for (data, label) in mnist if label == 6],dim = 0)[N:N+M//2].view(-1,28*28)\n",
    "test_data_anomaly = torch.cat([data for (data, label) in mnist if label != 6],dim = 0)[:M//2].view(-1,28*28)\n",
    "\n",
    "test_data = torch.cat([test_data_normal,test_data_anomaly], dim = 0) \n",
    "test_data_label = np.array([False]*(M//2)+[True]*(M//2)) # True indicates the anomaly data point\n",
    "\n",
    "print('training_image.shape:', training_data.shape)\n",
    "print('test_image.shape:', test_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fda4b5",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d265e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5x5 subplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "print('Normal data samples:')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[M//2+i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "print('Anomaly data samples:')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fda4b5",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### Reconstruction error-based anomaly detection [1/3]\n",
    "\n",
    "- 784차원의 데이터를 **encoder**를 이용하여 2차원의 **low dimensional space로 mapping**하고, **decoder**를 이용하여 복원하는 **autoencoder**를 학습함.\n",
    "\n",
    "- MLP를 기반으로 **encoder**와 **decoder**를 설계함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb787d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Autoencoder Model\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        \"\"\"First, we simply define the encoder and decoder archiectures.\n",
    "        \"\"\"\n",
    "\n",
    "        # Encoder: Map 784-dimensional image to 2-dimensional vector\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 128), \n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.Linear(128, 128), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "        # Decoder: reconstruct 784-dimensional image from 2-dimensional vector\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 128), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 128), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 28 * 28),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x) # encoding\n",
    "        decoded = self.decoder(encoded) # deconding\n",
    "        return decoded\n",
    "\n",
    "# Instantiate the autoencoder\n",
    "autoencoder = Autoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d97ba03",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Reconstruction error-based anomaly detection [2/3]\n",
    "\n",
    "- 위의 autoencoder를 training data에 대하여 **reconstruction error** $$\\text{min}\\frac{1}{N}\\sum^N_{i=1}\\|x_i-D(E(x_i)) \\|^2$$를 최소화 하도록 학습시킴. $E(\\cdot)$은 인코더이며, $D(\\cdot)$은 디코더임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450deabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(data,reconstructed_data):\n",
    "    \"\"\"Compute the reconstruction error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: original data (Nx784 torch matrix)\n",
    "    reconstructed_data: reconstructed data (Nx784 torch matrix)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    error: float\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" You can use functions in a package.\n",
    "    torch.sum(): https://pytorch.org/docs/stable/generated/torch.sum.html\n",
    "    torch.mean(): https://pytorch.org/docs/stable/generated/torch.sum.html\n",
    "    \"\"\"\n",
    "    \n",
    "    error = torch.mean(torch.sum(\"\"\" TODO \"\"\"), dim = 0) \n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "# (Optional) Use package\n",
    "\"\"\" You can use function in package: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "reconstruction_error = # TODO #\n",
    "\"\"\" \n",
    "\n",
    "# Training loop\n",
    "num_epoch = 5000\n",
    "training_data = training_data.to(device)\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    # Forward pass\n",
    "    reconstructed_data = autoencoder(training_data)\n",
    "    loss = reconstruction_error(reconstructed_data, training_data)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"\\r Epoch [{i+1}/{num_epoch}], Loss: {loss.item():.4f}\", end = ' ')\n",
    "\n",
    "last_loss = loss.item()\n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e44b4d0",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Reconstruction error-based anomaly detection [3/3]\n",
    "\n",
    "- 테스트 데이터에 대한 **reconstruction error**가 일정 이상으로 크다면, **outlier**로 판단함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ef2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = [] \n",
    "anomaly_score = [] \n",
    "threshhold = 3*last_loss\n",
    "\n",
    "for i in range(0,M):\n",
    "    error = reconstruction_error(test_data[i].view(1,-1),autoencoder(test_data[i].view(1,-1))).item() # Compute the likelihood of test data point\n",
    "    is_anomaly = threshhold < error  # If error is bigger than threshhold, is_anomaly should be True\n",
    "    \n",
    "    anomaly_score.append(error)\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e44b4d0",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027302e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5x5 subplot\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.title('error: '+str(np.round(anomaly_score[i],3)))\n",
    "    plt.axis('off')\n",
    "print('Normal data samples:')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[M//2+i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.title('error: '+str(np.round(anomaly_score[M//2+i],3)))\n",
    "    plt.axis('off')\n",
    "print('Anomaly data samples:')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fda4b5",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### Pre-trained autoencoder-based anomaly detection [1/3]\n",
    "\n",
    "- 784차원의 데이터를 2차원의 **low dimensional space로 mapping**하는 **pre-trained autoencoder**를 이용함.\n",
    "\n",
    "- 그러면, **low dimensional space**에서 Gaussian model 및 KDE 방법 적용 가능.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "**하지만, pre-trained autoencoder가 사전에 존재해야함.**\n",
    "\n",
    "- 이를 만족 시키기 위해, 임시로 **pre-trained autoencoder**를 좀 더 큰 데이터셋에서 따로 학습시킴.\n",
    "\n",
    "> **training_data_for_pretraining:** pre-trained deep learning model (autoencoder)를 학습하기 위한 데이터셋 ($5000 \\times 2$ torch matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_for_pretraining = torch.cat([data for (data, _) in mnist],dim = 0)[:5000].view(-1,28*28).to(device)\n",
    "\n",
    "# Instantiate the autoencoder\n",
    "pre_trained_autoencoder = Autoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(pre_trained_autoencoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450deabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-training loop\n",
    "num_epoch = 5000\n",
    "training_data_for_AE = pre_trained_autoencoder.to(device)\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    # Forward pass\n",
    "    reconstructed_data = pre_trained_autoencoder(training_data_for_pretraining)\n",
    "    loss = reconstruction_error(reconstructed_data, training_data_for_pretraining)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"\\r Epoch [{i+1}/{num_epoch}], Loss: {loss.item():.4f}\", end = ' ')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Pre-trained autoencoder-based anomaly detection [2/3]\n",
    "\n",
    "- **Pre-trained autoencoder**를 이용하여, **data**를 **low-dimensional space**에 mapping함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7003e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pre_trained_autoencoder.encoder(training_data.to(device)).cpu().detach().numpy()\n",
    "test_data = pre_trained_autoencoder.encoder(test_data.to(device)).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot training data in green\n",
    "plt.scatter(training_data[:, 0], training_data[:, 1], s = 20,c='green', label='Training Data')\n",
    "# Plot test data\n",
    "plt.scatter(test_data[:, 0], test_data[:, 1], c=[('red' if v else 'blue') for v in test_data_label])\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "\n",
    "### Pre-trained autoencoder-based anomaly detection [3/3]\n",
    "\n",
    "- **Low-dimensional space**에서 **Kernel density estimation**를 사용하여 anomaly detection 수행 (**Gaussian model**을 사용해도 됨)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = []\n",
    "threshhold = 0.05\n",
    "\n",
    "for i in range(0,M):\n",
    "    likelihood = \"\"\" TODO \"\"\"\n",
    "    is_anomaly = likelihood < threshhold\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(np.min(test_data[:,0]), np.max(test_data[:,0]), 30), np.linspace(np.min(test_data[:,1]), np.max(test_data[:,1]), 30))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = np.array([kernel_density_estimation(training_data, p) for p in points])\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), \n",
    "                marker='o' if anomaly_predict[i]==test_data_label[i] else 'x')\n",
    "\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "# 3. Anomaly detection with likelihood ratio\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 3.1. Overview\n",
    "\n",
    "**Problem: 데이터에 대한 likelihood score의 대부분은 background statistics에 큰 영향을 받음.**\n",
    "\n",
    "- 예를 들어 아래와 같은 이미지의 경우, background인 zero pixel이 대부분의 statistics을 차지함.\n",
    "\n",
    "<img src=\"./images/anomaly_motivation.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 그러면, 충분한 zero pixel을 포함하는 **anomaly data**와 **normal data**의 **likelihood 차이가 적을 것**. 즉, anomaly detection이 어려움.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Likelihood ratio**는 **background information을 anomaly detection에서 무시**하기 위해 제안된 비교적 최근 프레임 워크.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 3.2. Method\n",
    "\n",
    "**Likelihood ratio는 다음과 같은 가정을 기반으로함.**\n",
    "\n",
    "- **(Assumption 1)** 각각의 데이터 $x$는 **semantic component**인 $x_S$와 **background componen**t인 $x_B$로 분해될 수 있음 ($p(x)=p(x_B)p(x_S)$).\n",
    "\n",
    "- **(Assumption 2)** 데이터에 **noise perturbation을 추가**해도, **background component**에 대한 **statistics**는 크게 변하지 않음.  \n",
    "\n",
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "**위와 같은 가정에서, likelihood ratio는 다음과 같은 프레임워크를 가짐.**\n",
    "\n",
    "- **Ingredient: Noise perturbed data**에 대한 분포를 나타내는 $\\tilde{p}({x})$와 기존의 **training data**에 대한 분포를 나타내는 $p(x)$.\n",
    "\n",
    "- **Likelihood ratio**은 주어진 data $x'$에 대하여, 두 분포에 대한 **likelihood ratio**를 다음과 같이 정의함.\n",
    "\n",
    "$$\\text{LLR}(x')=\\frac{p(x')}{\\tilde{p}(x')}=\\frac{p({x'}_B)p({x'}_S)}{\\tilde{p}({x'}_B)\\tilde{p}({x'}_S)}\\approx\\frac{p({x'}_S)}{\\tilde{p}({x'}_S)},\\qquad \\text{(Assumption 2) } \\tilde{p}({x'}_B)\\approx p({x'}_B)$$\n",
    "\n",
    "- 여기서,  $\\text{LLR}(x')$는 **semantic component**만을 기반으로 **likelihood score**를 제공함.\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 3.3. Implementation\n",
    "\n",
    "1. **Noise perturbed data 대한 분포를 나타내는 $\\tilde{p}({x})$와 기존의 training data에 대한 분포를 나타내는 $p(x)$를 학습.** \n",
    "\n",
    "2. 테스트 데이터에 대해 **$\\text{LLR}(x')={p(x')}/{\\tilde{p}(x')}$를 계산하면, background statistics는 무시하고 anomaly detection 수행 가능.**\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Define perturbed data\n",
    "\n",
    "- 우선, **pertured data distribution** $\\tilde{p}(x)$를 학습하기 위한 **perturbed dataset** $\\{\\tilde{x}_1,\\ldots,\\tilde{x}_N\\}$을 정의함. 이는 단순히 training data에 **random noise**를 추가하여 얻을 수 있음.\n",
    "\n",
    "> **perturbed_training_data**: Noise perturbation이 추가된 perturebed dataset ($N \\times 2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f54ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" You can sample noise from various distribution.\n",
    "normal distribution: https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html\n",
    "uniform distribution: https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html\n",
    "(recomendation) np.random.normal(# TODO #)\n",
    "\"\"\" \n",
    "\n",
    "perturbed_training_data =  \"\"\" TODO \"\"\" # add perturbation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "### Likelihood ratio-based Anomaly detection with Gaussian Model [1/2]\n",
    "\n",
    "- **Perturbed data distribution**인 $\\tilde{p}(x)$와, **normal training data distribution**인 $p(x)$를 각각 학습해야함.\n",
    "\n",
    "- **Gaussian model**을 이용하여 각각의 **data distribution**을 표현할 수 있음. $$p(x)=p(x;\\mu,\\Sigma),\\qquad \\tilde{p}(x)=p(x;\\tilde{\\mu},\\tilde{\\Sigma})$$ $\\mu,\\Sigma$는 normal training data $\\{x_1,\\ldots, x_N\\}$에 대한 분포를 표현하며, $\\tilde{\\mu},\\tilde{\\Sigma}$는 perturbed data $\\{\\tilde{x}_1,\\ldots, \\tilde{x}_N\\}$에 대한 분포를 표현함. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" You can use fit_Gaussian_parameter() function.\n",
    "mean, cov: mean and covariance parameters of normal training data distribution.\n",
    "perturbed_mean, perturbed_cov: mean and covariance parameters of perturbed training data distribution.\n",
    "\"\"\" \n",
    "\n",
    "# The parameters of Gaussian model trained on training data\n",
    "mean, cov = \"\"\" TODO \"\"\"\n",
    "\n",
    "# The parameters of Gaussian model trained on perturbed data\n",
    "perturbed_mean, perturbed_cov = \"\"\" TODO \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b677bdcc",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Likelihood ratio-based Anomaly detection with Gaussian Model [2/2]\n",
    "\n",
    "- 각각 학습된 모델을 기반으로, $\\text{LLR}(x')={p(x')}/{\\tilde{p}(x')}$를 계산할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = []\n",
    "threshhold = 1.5\n",
    "\n",
    "for i in range(M):\n",
    "\n",
    "    # Compute LLR score\n",
    "    p_x = \"\"\" TODO \"\"\"\n",
    "    p_perturbed_x = \"\"\" TODO \"\"\"\n",
    "    LLR_x = \"\"\" TODO \"\"\"\n",
    "\n",
    "    # Is anomaly?\n",
    "    is_anomaly = LLR_x < threshhold\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4612d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(np.min(test_data[:,0]), np.max(test_data[:,0]), 30), np.linspace(np.min(test_data[:,1]), np.max(test_data[:,1]), 30))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = np.array([compute_likelihood_Gaussian(p, mean, cov)/compute_likelihood_Gaussian(p, perturbed_mean, perturbed_cov)\n",
    "                       for p in points])\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "markers = np.where(anomaly_predict==test_data_label, \"o\", \"x\").tolist()\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), \n",
    "                marker='o' if anomaly_predict[i]==test_data_label[i] else 'x')\n",
    "\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec146e83",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### Likelihood ratio with Autoregressive model\n",
    "\n",
    "우리는 $p(x)$를 표현하기 위해 간단한 모델을 사용했음. 하지만, **sequence**와 같은 data $x=\\{x^{(1)},\\ldots,x^{(L)}\\}$에 대하여, $p(x)$를 좀 더 복잡하게 복잡하게 모델링 해야할 수 있음.\n",
    "\n",
    "- **Sequence data** $x$에 대한 **likelihood ratio** 기반의 anomaly detection을 수행함.\n",
    "\n",
    "- 좀 더 복잡한 **autoregressive model** $p(x)=p(x^{(1)})p(x^{(2)}|x^{(1)})\\cdots p(x_L|x^{(1)},\\ldots,x^{(L-1)})$를 이용하여 data distribution을 표현함.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29a7996d",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Likelihood ratio with Autoregressive model [1/4]\n",
    "\n",
    "- 우선, A부터 D로 구성된 **sequence** 형태의 data $x$를 정의함.\n",
    "\n",
    "- **Normal data**와 **abnormal data**는 sequence에서 다른 패턴을 가짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic categorical sequence dataset with dependencies\n",
    "def generate_normal_categorical_sequence(length=10):\n",
    "    \n",
    "    # Define the transition probabilities\n",
    "    # If current token is 'A', the next token will be 'A' (with p = 0.7) or 'B' (with p = 0.3).\n",
    "    # e.g., AAABBCCCCDABBBB...\n",
    "    transition_probabilities = {\n",
    "        'A': {'A': 0.7, 'B': 0.3, 'C': 0.0, 'D': 0.0},\n",
    "        'B': {'A': 0.0, 'B': 0.7, 'C': 0.3, 'D': 0.0},\n",
    "        'C': {'A': 0.0, 'B': 0.0, 'C': 0.7, 'D': 0.3},\n",
    "        'D': {'A': 0.3, 'B': 0.0, 'C': 0.0, 'D': 0.7}\n",
    "    }\n",
    "\n",
    "    categories = ['A', 'B', 'C', 'D']\n",
    "    sequence = []\n",
    "    current_category = np.random.choice(categories)  # Start with a random category\n",
    "    sequence.append(current_category)\n",
    "\n",
    "    for _ in range(length - 1):\n",
    "        next_category = np.random.choice(categories, p=[transition_probabilities[current_category][c] for c in categories])\n",
    "        sequence.append(next_category)\n",
    "        current_category = next_category\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "\n",
    "# Generate a synthetic categorical sequence dataset with dependencies\n",
    "def generate_abnormal_categorical_sequence(length=10):\n",
    "    \n",
    "    # Define the transition probabilities\n",
    "    # e.g., AAAADDCBBADDD....\n",
    "    transition_probabilities = {\n",
    "        'A': {'A': 0.7, 'B': 0.1, 'C': 0.1, 'D': 0.1},\n",
    "        'B': {'A': 0.1, 'B': 0.7, 'C': 0.1, 'D': 0.1},\n",
    "        'C': {'A': 0.1, 'B': 0.1, 'C': 0.7, 'D': 0.1},\n",
    "        'D': {'A': 0.1, 'B': 0.1, 'C': 0.1, 'D': 0.7}\n",
    "    }\n",
    "\n",
    "    categories = ['A', 'B', 'C', 'D']\n",
    "    sequence = []\n",
    "    current_category = np.random.choice(categories)  # Start with a random category\n",
    "    sequence.append(current_category)\n",
    "\n",
    "    for _ in range(length - 1):\n",
    "        next_category = np.random.choice(categories, p=[transition_probabilities[current_category][c] for c in categories])\n",
    "        sequence.append(next_category)\n",
    "        current_category = next_category\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "\n",
    "print('Example of normal x: ', generate_normal_categorical_sequence(50))\n",
    "print('\\nExample of abnormal x: ', generate_abnormal_categorical_sequence(50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "383bf15d",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "위의 데이터를, **torch**가 다룰 수 있는 형태의 **dataset**으로 정의함.\n",
    "\n",
    "- **50 length를 가진 1000개의 sequence**를 **training dataset**으로 정의하며, **50 length를 가진 10개의 sequence를 test dataset**으로 정의함.\n",
    "\n",
    "- 각각의 alphabet에 one hot operation을 적용하여 4-dimensional vector로 만듬.\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **training_data**: ($1000 \\times 50 \\times 4$)의 torch tensor\n",
    "\n",
    "> **test_data**: ($10 \\times 50 \\times 4$)의 torch tensor (1~5은 normal, 5~10은 outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map categories to one-hot vectors\n",
    "category_to_one_hot = {'A': [1, 0, 0, 0], 'B': [0, 1, 0, 0], 'C': [0, 0, 1, 0], 'D': [0, 0, 0, 1]}\n",
    "\n",
    "\n",
    "\n",
    "# Generate a training data\n",
    "training_data = []\n",
    "N = 1000\n",
    "\n",
    "for _ in range(N):\n",
    "    sequence = generate_normal_categorical_sequence(length=50)\n",
    "    training_data.append(sequence)\n",
    "\n",
    "# Convert the sequences to one-hot encoded vectors\n",
    "training_data = torch.tensor([\n",
    "    [category_to_one_hot[category] for category in sequence]\n",
    "    for sequence in training_data\n",
    "]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a test data\n",
    "test_data_orig = []\n",
    "M = 10\n",
    "\n",
    "for _ in range(M//2):\n",
    "    sequence = generate_normal_categorical_sequence(length=50)\n",
    "    test_data_orig.append(sequence)\n",
    "for _ in range(M//2):\n",
    "    sequence = generate_abnormal_categorical_sequence(length=50)\n",
    "    test_data_orig.append(sequence)\n",
    "\n",
    "# Convert the sequences to one-hot encoded vectors\n",
    "test_data = torch.tensor([\n",
    "    [category_to_one_hot[category] for category in sequence]\n",
    "    for sequence in test_data_orig\n",
    "]).to(device)\n",
    "test_data_label = np.array([False]*(M//2)+[True]*(M//2)) # True indicates the anomaly data point\n",
    "\n",
    "print('training_data.shape: ',training_data.shape)\n",
    "print('test_data.shape: ',test_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0044b724",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Likelihood ratio with Autoregressive model [2/4]\n",
    "\n",
    "- **LSTM** 기반의 **autoregressive** 모델 $p(x)=p(x^{(1)})p(x^{(2)}|x^{(1)})\\cdots p(x_L|x^{(1)},\\ldots,x^{(L-1)})$ 을 정의 및 훈련함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the LSTM model for generative modeling with log softmax\n",
    "class LSTMGenerativeModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMGenerativeModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size, batch_first = True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        out = torch.nn.functional.log_softmax(out, dim = -1)\n",
    "        return out, hidden\n",
    "    \n",
    "# Define hyperparameters\n",
    "input_size = 4  # Number of categories (A, B, C, D)\n",
    "hidden_size = 64  # Size of the LSTM hidden state\n",
    "output_size = 4  # Number of categories (A, B, C, D)\n",
    "learning_rate = 0.003\n",
    "\n",
    "# Initialize LSTMGenerativeModel\n",
    "model = LSTMGenerativeModel(input_size, hidden_size, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize hidden state of LSTM for the entire batch\n",
    "    hidden = (torch.zeros(1, 1000, hidden_size).to(device), torch.zeros(1, 1000, hidden_size).to(device))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for time_step in range(49):\n",
    "        batch_input = training_data[:, time_step, :].unsqueeze(1).float()  # Shape: (batch_size, 1, input_size)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output, hidden = model(batch_input, hidden)\n",
    "        batch_target = training_data[:, time_step+1, :].argmax(dim=-1)  # Shape: (batch_size)\n",
    "\n",
    "        # Calculate loss for this time step and accumulate\n",
    "        loss += criterion(output.squeeze(1), batch_target)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss at the end of each epoch\n",
    "    print(f'\\r Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}', end = ' ')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0044b724",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Likelihood ratio with Autoregressive model [3/4]\n",
    "\n",
    "- **Autoregressive** 모델을 **perturbed data**에 대하여 훈련함 (perturbed data distribution $\\tilde{p}(x)$를 훈련).\n",
    "\n",
    "- 우선, **perturbed training data** 부터 정의함.\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **perturbed training data**: training data에 random perturbation을 가함 (one-hot index의 location을 랜덤으로 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bfda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define perturbed data\n",
    "\n",
    "def introduce_perturbations(tensor, perturbation_probability=0.1):\n",
    "    perturbed_tensor = tensor.clone()  # Create a copy of the input tensor\n",
    "    \n",
    "    # Iterate through the tensor and introduce perturbations\n",
    "    for sequence in perturbed_tensor:\n",
    "        for t in range(sequence.size(0)):  # Assuming the first dimension is the time steps\n",
    "            if random.random() < perturbation_probability:\n",
    "                # Find the current location of '1' in the sequence\n",
    "                current_location = torch.argmax(sequence[t])\n",
    "                \n",
    "                # Generate a new random location for '1' that is different from the current location\n",
    "                new_location = current_location\n",
    "                while new_location == current_location:\n",
    "                    new_location = random.randint(0, sequence.size(1) - 1)  # Assuming the second dimension is the one-hot encoding\n",
    "                \n",
    "                # Move '1' to the new location\n",
    "                sequence[t][current_location] = 0\n",
    "                sequence[t][new_location] = 1\n",
    "    \n",
    "    return perturbed_tensor\n",
    "\n",
    "\n",
    "\n",
    "# Introduce perturbations into your training_data tensor\n",
    "perturbed_training_data = introduce_perturbations(training_data, perturbation_probability=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTMGenerativeModel\n",
    "perturbed_model = LSTMGenerativeModel(input_size, hidden_size, output_size)\n",
    "perturbed_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(perturbed_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize hidden state for the entire batch\n",
    "    hidden = (torch.zeros(1, 1000, hidden_size).to(device), torch.zeros(1, 1000, hidden_size).to(device))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for time_step in range(49):\n",
    "        batch_input = perturbed_training_data[:, time_step, :].unsqueeze(1).float()  # Shape: (batch_size, 1, input_size)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output, hidden = perturbed_model(batch_input, hidden)\n",
    "        batch_target = perturbed_training_data[:, time_step+1, :].argmax(dim=-1)  # Shape: (batch_size)\n",
    "\n",
    "        # Calculate loss for this time step and accumulate\n",
    "        loss += criterion(output.squeeze(1), batch_target)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss at the end of each epoch\n",
    "    print(f'\\r Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}', end = ' ')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0044b724",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Likelihood ratio with Autoregressive model [4/4]\n",
    "\n",
    "- **perturbed model**과 **model**을 기반으로, **test data**에 대한 **likelihood ratio**를 계산함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d617283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each sequence in the test data\n",
    "for i, sequence in enumerate(test_data):\n",
    "    # Initialize hidden state for the sequence\n",
    "    hidden = (torch.zeros(1, 1, hidden_size).to(device), torch.zeros(1, 1, hidden_size).to(device))\n",
    "\n",
    "\n",
    "    # Initialize likelihood\n",
    "    sequence_likelihood = 0.0\n",
    "    \n",
    "    for time_step in range(0,49):\n",
    "        category_vector = sequence[time_step].view(1, 1, -1).float()  # Reshape for LSTM\n",
    "        output, hidden = model(category_vector, hidden)\n",
    "        target = torch.argmax(sequence[time_step+1].view(1, 1, -1), dim=-1)\n",
    "        sequence_likelihood += -criterion(output.squeeze(0), target.squeeze(0)).item()  # Use negative log-likelihood\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Initialize likelihood for perturbed\n",
    "    sequence_likelihood_perturbed = 0.0\n",
    "\n",
    "    for time_step in range(0,49):\n",
    "        category_vector = sequence[time_step].view(1, 1, -1).float()  # Reshape for LSTM\n",
    "        output, hidden = perturbed_model(category_vector, hidden)\n",
    "        target = torch.argmax(sequence[time_step+1].view(1, 1, -1), dim=-1)\n",
    "        sequence_likelihood_perturbed += -criterion(output.squeeze(0), target.squeeze(0)).item()  # Use negative log-likelihood\n",
    "        \n",
    "        \n",
    "    LLR = np.exp(sequence_likelihood)/np.exp(sequence_likelihood_perturbed)\n",
    "    \n",
    "    print(''.join(test_data_orig[i]), ' label:','outlier' if test_data_label[i] else 'normal', ' LLR:', LLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d40e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9 (default, Mar 10 2023, 16:46:00) \n[GCC 8.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
