{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abedafe6",
   "metadata": {},
   "source": [
    "# 1. Generator-based anomaly detection \n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 1.1. Overview\n",
    "\n",
    "**Data distribution $p(x)$를 학습하여, 새로 주어진 target data가 outlier인지 판단함.**\n",
    "\n",
    "1. **Data distribution** $p(x)$를 표현하는 **모델**을 학습함. \n",
    "\n",
    "2. 만약, 새로 주어진 **target data** $x'$이 **data distribution**에 속할 확률이 낮다면 ($p(x')<\\epsilon$), outlier로 판단함.\n",
    "\n",
    "<img src=\"./generator_example.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 1.2. Method\n",
    "\n",
    "**어떻게 data distribution $p(x)$를 표현하는 모델을 학습할 수 있는가?** \n",
    "\n",
    "### 1.2.1. Gaussian model\n",
    "\n",
    "**Data distribution $p(x)$를 다루기 쉬운 Gaussian distribution으로 가정하는 프레임워크.**\n",
    "\n",
    "1. 주어진 데이터셋에 대한 분포를 표현할 수 있는 **Gaussian distribution**를 추정함. \n",
    "\n",
    "2. 만약 주어진 **target data**가 **추정된 Gaussian distribution**에서 얻어질 확률이 작다면 ($p(x')<\\epsilon$), 해당 데이터를 **outlier**로 판단함.\n",
    "\n",
    "<img src=\"./gaussian_example.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### 1.2.2. Kernel density estimation (KDE)\n",
    "\n",
    "**Data distribution $p(x)$를 모든 training data point에 대한 point-wise kernel을 이용하여 표현함.**\n",
    "\n",
    "1. 모든 training data로 표현된 **kernel density**로부터 **target data**가 얻어질 확률을 값을 계산함.\n",
    "\n",
    "2. 계산된 확률 값이 일정 이하라면, 해당 데이터를 **outlier**로 판단함.\n",
    "\n",
    "<img src=\"./kde_example.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2371dc1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 1.3. Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73c7956c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Install / Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library \n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install --upgrade setuptools\n",
    "!pip3 install numpy\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acded75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Define toy dataset (2-dimensional data)\n",
    "\n",
    "- 간단한 예시로서, **2-dimensional toy dataset**에서 anomaly detection을 수행함. \n",
    "\n",
    "- **training data는 총 100개**이며, **test data는 20개** (10개는 anomaly, 남은 10개는 normal data).\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Variable list:**\n",
    "\n",
    "> **N**: training data의 개수 (=100).\n",
    "\n",
    "> **M**: test data의 개수 (=20).\n",
    "\n",
    "> **training_data**: training dataset에 대한 numpy matrix ($N \\times 2$).\n",
    "\n",
    "> **test_data**: test dataset에 대한 numpy matrix ($M \\times 2$).\n",
    "\n",
    "> **test_data_label**: 각각의 test data가 anomaly data인가에 대한 ground truth (boolean array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # Number of training data\n",
    "M = 20 # Number of test data\n",
    "\n",
    "\n",
    "# Generate some normal data points\n",
    "np.random.seed(0)\n",
    "mean_normal = np.array([5, 5])\n",
    "cov_normal = np.array([[1, 0.5], [0.5, 1]])\n",
    "training_data = np.random.multivariate_normal(mean_normal, cov_normal, N) # normal data\n",
    "\n",
    "\n",
    "# Generate some test data points\n",
    "mean_anomaly = np.array([7, 7])\n",
    "cov_anomaly = np.array([[2, -1], [-1, 2]])\n",
    "test_data_normal = np.random.multivariate_normal(mean_normal, cov_normal, M//2) # normal data\n",
    "test_data_anomaly = np.random.multivariate_normal(mean_anomaly, cov_anomaly, M//2) # anomaly data\n",
    "test_data = np.concatenate([test_data_normal,test_data_anomaly], axis = 0) \n",
    "test_data_label = np.array([False]*(M//2)+[True]*(M//2)) # True indicates the anomaly data point\n",
    "\n",
    "print('training_data.shape:', training_data.shape)\n",
    "print('target_data.shape:', test_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot training data in green\n",
    "plt.scatter(training_data[:, 0], training_data[:, 1], s = 20,c='green', label='Training Data')\n",
    "# Plot test data\n",
    "plt.scatter(test_data[:, 0], test_data[:, 1], c=[('red' if v else 'blue') for v in test_data_label])\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "### Anomaly detection with Gaussian model [1/3]\n",
    "\n",
    "**Data distribution을 Gaussian distribution $p(x;\\mu,\\Sigma)$으로 표현함.**\n",
    "\n",
    "- **Gaussian distribution** $p(x;\\mu,\\Sigma)$의 파라미터인 **mean**과 **covariance** $(\\mu,\\Sigma)$를 **training data**로 계산해야함.\n",
    "\n",
    "- **Mean**과 **covariance** $(\\mu,\\Sigma)$는 다음과 같이 계산 가능함.\n",
    "$$\\mu=\\frac{1}{N}\\sum_{n=1}^N x_n, \\qquad \\Sigma=\\frac{1}{N}\\sum_{n=1}^N (x_n-\\mu)(x_n-\\mu)^T$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba10fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_Gaussian_parameter(data):\n",
    "    \"\"\"Obtain parameters of Gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: N x 2 array (training data, where row indicies each data point)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean: single float\n",
    "    covariance: single float\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" You can use functions in libraries.\n",
    "    mean: https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "    covariance: https://numpy.org/doc/stable/reference/generated/numpy.cov.html\n",
    "    \"\"\" \n",
    "\n",
    "    number_of_data = data.shape[0]\n",
    "\n",
    "    # Compute mean and covariance using numpy module\n",
    "    mean = np.mean(data, axis = 0)\n",
    "    cov = np.cov(data, rowvar = False)\n",
    "\n",
    "    # (Optional) Compute mean parameter directly\n",
    "    mean = 0\n",
    "    for i in range(number_of_data):\n",
    "        mean = mean + data[i]\n",
    "    mean = mean / number_of_data\n",
    "\n",
    "    # (Optional) Compute covariance parameter directly\n",
    "    cov = np.zeros((2, 2))\n",
    "    for i in range(number_of_data):\n",
    "        de_mean = data[i] - mean\n",
    "        de_mean = de_mean.reshape(2,1) # Convert 2-dimensional vector to (2x1) matrix\n",
    "        cov = cov + np.dot(de_mean,de_mean.T)\n",
    "    cov = cov / number_of_data\n",
    "\n",
    "    return mean, cov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Anomaly detection with Gaussian model [2/3]\n",
    "\n",
    "- **Gaussian distribution**의 파라미터가 주어졌을 때, 테스트 데이터 $x'$의 **likelihood**는 다음과 같음. $$p(x';\\mu,\\Sigma)=\\frac{1}{\\sqrt{(2\\pi)^{2}\\det(\\Sigma)}}\\exp(-\\frac{1}{2}(x'-\\mu)^T\\Sigma^{-1}(x'-\\mu))$$\n",
    "\n",
    "- 계산된 **likelihood** $p(x';\\mu,\\Sigma)$로 데이터 $x'$가 **outlier**인지 판단할 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca118f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood_Gaussian(target, mean, covariance):    \n",
    "    \"\"\"Compute the likelihood of target given Gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target: 2-dimensional array (single target data)\n",
    "    mean: \n",
    "    covariance:\n",
    "    threshhold:\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_anomaly: Bool\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" You can use function in package.\n",
    "    multivariate_normal.pdf: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html\n",
    "    (recomendation) anomaly_score = multivariate_normal.pdf(# TODO #)\n",
    "    \"\"\" \n",
    "    \n",
    "    # Compute likelihood using scipy module given mean and covariance\n",
    "    likelihood = multivariate_normal.pdf(target, mean, covariance)\n",
    "\n",
    "    # (Optional) Compute likelihood directly given mean and covariance\n",
    "    det_cov = np.linalg.det(covariance)\n",
    "    inv_cov = np.linalg.inv(covariance)\n",
    "    exponent = -0.5 * np.dot(np.dot((target - mean).T, inv_cov), (target - mean))\n",
    "    coeff = 1 / (np.sqrt((2 * np.pi) ** 2 * det_cov))\n",
    "    likelihood = coeff * np.exp(exponent)\n",
    "\n",
    "    return likelihood   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a41254ae",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Anomaly detection with Gaussian model [3/3]\n",
    "\n",
    "1. training data distribution을 표현하는 **Gaussian model의 파라미터**를 얻음.\n",
    "\n",
    "2. **test data에 대한 likelihood**를 위의 모델로부터 계산함\n",
    "\n",
    "3. 만약, 주어진 데이터의 **likelihood**가 threshhold $\\epsilon$보다 작다면 $(p(x')<\\epsilon )$, outlier로 판단.\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **anomaly_predict**: 각각의 test data가 anomaly data인가에 대한 prediction (boolean list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca118f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain mean and covariance parameter through training data\n",
    "mean, cov = fit_Gaussian_parameter(training_data)\n",
    "threshhold = 0.03\n",
    "\n",
    "# Predict the label of test data (is anomaly?)\n",
    "anomaly_predict = []\n",
    "\n",
    "for i in range(0,M):\n",
    "    likelihood = compute_likelihood_Gaussian(test_data[i], mean, cov) # Compute the likelihood of test data point\n",
    "\n",
    "    is_anomaly = likelihood < threshhold  # If likelihood is smaller than threshhold, is_anomaly should be True\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediciton: ', anomaly_predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb28d3b9",
   "metadata": {},
   "source": [
    "- 예측한 결과에 대한 performance를 F1 score 및 accuracy를 이용하여 측정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "\"\"\" You can use functions in a package.\n",
    "sklearn.metrics.accuracy: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "sklearn.metrics.F1: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\"\"\"\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(1, 10, 100), np.linspace(0, 10.5, 100))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = multivariate_normal.pdf(points, mean=mean, cov=cov)\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "markers = np.where(anomaly_predict==test_data_label, \"o\", \"x\").tolist()\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), marker=markers[i])\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### Anomaly detection with Kernel Density Estimation [1/2]\n",
    "\n",
    "**Data distribution을 각각의 training data에 대한 point-wise kernel을 이용하여 표현함.**\n",
    "\n",
    "- Data point $x'$에 대하여 **kernel density estimation**의 **likelihood**는 다음과 같이 정의됨. $$p(x')=\\frac{1}{N}\\sum^{N}_{i} k(x_i,x'),$$ $\\{x_1,\\ldots, x_N\\}$은 training data이며 $k(\\cdot,\\cdot)$는 kernel. \n",
    "\n",
    "- 각각의 kernel $k(x_i,x')$은 주어진 training data $x_i$를 mean으로 사용하는 Gaussian distribution을 기반으로 정의함. $$k(x_i,x')=\\frac{1}{(2\\pi \\ell^2)}\\exp(-\\frac{1}{2 \\ell^2}\\|x-x_i \\|^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ef2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_density_estimation(dataset, target, bandwidth = 1, threshhold=0.1):\n",
    "    \"\"\"Compute the likelihood of the target using training data based on kernel density estimation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: N x 2 array (training data)\n",
    "    target: 2-dimensional array array (single target data)\n",
    "    bandwidth: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    likelihood: float\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def kernel(data, target, bandwidth):\n",
    "        \"\"\"Compute the kernel value of the target.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: 2-dimensioanl array (single training data)\n",
    "        target: 2-dimensional array (single target data)\n",
    "        bandwidth: float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        k: float\n",
    "        \"\"\"\n",
    "\n",
    "        frac = 1/(2*np.pi*bandwidth)\n",
    "        norm_sq = np.sum((target - data)**2) # ||x - x_i||^2\n",
    "        k = frac * np.exp(-1/(2 * bandwidth**2) * norm_sq)\n",
    "        return k\n",
    "\n",
    "\n",
    "    likelihood = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        k = kernel(dataset[i], target, bandwidth)\n",
    "        likelihood = likelihood + k\n",
    "\n",
    "    likelihood = likelihood / N\n",
    "\n",
    "    return likelihood\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Anomaly detection with Kernel Density Estimation [2/2]\n",
    "\n",
    "1. **test data에 대한 likelihood**를 **kernel density estimation**으로부터 계산함\n",
    "\n",
    "2. 만약, 주어진 데이터의 **likelihood**가 threshhold $\\epsilon$보다 작다면 $(p(x')<\\epsilon )$, outlier로 판단."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ef2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = [] \n",
    "threshhold = 0.03\n",
    "\n",
    "for i in range(0,M):\n",
    "    likelihood = kernel_density_estimation(training_data, test_data[i]) # Compute the likelihood of test data point\n",
    "    \n",
    "    is_anomaly = likelihood < threshhold # If likelihood is smaller than threshhold, is_anomaly should be True\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and F1 score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65614a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(1, 10, 100), np.linspace(0, 10.5, 100))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = np.array([kernel_density_estimation(training_data, p) for p in points])\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "markers = np.where(anomaly_predict==test_data_label, \"o\", \"x\").tolist()\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), marker=markers[i])\n",
    "\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "# 2. Anomaly detection with Autoencoder\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 2.1. Overview \n",
    "\n",
    "**Problem: Gaussian model 및 KDE와 같은 방법은 이미지 (e.g., 28 x 28 = 784 dimensional data)와 같은 **high-dimensional data**에 적용하기 어려움.**\n",
    "\n",
    "- **High-dimensional image**를 **low dimension vector** (e.g., 2-dimensional vector)로 mapping하는 **Autoencoder**를 이용하여 문제를 해결함.\n",
    "\n",
    "- **Autoencoder**는 neural network로 구성된 **encoder**와 **decoder**로, 데이터를 **압축 및 복원**함.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2. Method\n",
    "\n",
    "### 2.2.1. Autoencoder-based anomaly detection.\n",
    "\n",
    "1. **Autoencoder**를 **training data**에 대하여 **reconstruction error**를 최소화 하도록 학습시킴.\n",
    "\n",
    "2. 만약, 새로 주어진 테스트 데이터 $x'$에 대하여 **reconstruction error**가 일정 이상이면, outlier로 판단함. \n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"./autoencoder_1.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### 2.2.2. Pre-trained autoencoder-based anomaly detection.\n",
    "\n",
    "\n",
    "\n",
    "1. 데이터에 대한 **pre-trained auto encoder**를 가지고 있다면, **high-dimensional image**를 **low dimension vector** (e.g., 2-dimensional vector) 로 mapping할 수 있음. \n",
    "\n",
    "3. 그러면, 이미지들이 mapping된 **low dimensional space**에서 **Gaussian model** 및 **KDE**를 적용하여 쉽게 **anomaly detection**을 수행할 수 있음.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"./autoencoder.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2371dc1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 2.3. Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73c7956c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Install / Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acded75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Import dataset\n",
    "\n",
    "- **training image는 1000개**이며, **test image는 100개** (50개는 normal data, 나머지 50개는 anomaly data). \n",
    "\n",
    "- 각각의 image는 $28\\times 28=784$ **pixels**를 가졌음.\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **N**: training data의 개수 (=1000).\n",
    "\n",
    "> **M**: test data의 개수 (=100).\n",
    "\n",
    "> **training_data:** 숫자 6에 대한 손글씨 데이터 ($N \\times 784$)\n",
    "\n",
    "> **test_image:** 숫자 6에 대한 손글씨 데이터 ($(M/2) \\times 784$)와 이외의 숫자에 대한 손글씨 데이터 ($(M/2) \\times 784$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a149f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "N = 1000\n",
    "M = 100\n",
    "\n",
    "training_data = torch.cat([data for (data, label) in mnist if label == 6],dim = 0)[:N].view(-1,28*28)\n",
    "test_data_normal = torch.cat([data for (data, label) in mnist if label == 6],dim = 0)[N:N+M//2].view(-1,28*28)\n",
    "test_data_anomaly = torch.cat([data for (data, label) in mnist if label != 6],dim = 0)[:M//2].view(-1,28*28)\n",
    "\n",
    "test_data = torch.cat([test_data_normal,test_data_anomaly], dim = 0) \n",
    "test_data_label = np.array([False]*(M//2)+[True]*(M//2)) # True indicates the anomaly data point\n",
    "\n",
    "print('training_image.shape:', training_data.shape)\n",
    "print('test_image.shape:', test_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fda4b5",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d265e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5x5 subplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "print('Normal data samples:')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[M//2+i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "print('Anomaly data samples:')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fda4b5",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### Autoencoder-based anomaly detection [1/3]\n",
    "\n",
    "- 784차원의 데이터를 **encoder**를 이용하여 2차원의 **low dimensional space로 mapping**하고, **decoder**를 이용하여 복원하는 **autoencoder**를 학습함.\n",
    "\n",
    "- MLP를 기반으로 **encoder**와 **decoder**를 설계함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb787d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Autoencoder Model\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        \"\"\"First, we simply define the encoder and archiecture.\n",
    "        \"\"\"\n",
    "\n",
    "        # Encoder: Map 784-dimensional image to 2-dimensional vector\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 128), \n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.Linear(128, 128), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "        # Decoder: reconstruct 784-dimensional image from 2-dimensional vector\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 128), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 128), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 28 * 28),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x) # encoding\n",
    "        decoded = self.decoder(encoded) # deconding\n",
    "        return decoded\n",
    "\n",
    "# Instantiate the autoencoder\n",
    "autoencoder = Autoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d97ba03",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Autoencoder-based anomaly detection [2/3]\n",
    "\n",
    "- 위의 autoencoder를 training data에 대하여 **reconstruction error** $$\\text{min}\\frac{1}{N}\\sum^N_{i=1}\\|x_i-D(E(x_i)) \\|^2$$를 최소화 하도록 학습시킴. $E(\\cdot)$은 인코더이며, $D(\\cdot)$은 디코더임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450deabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(data,reconstructed_data):\n",
    "    \"\"\"Compute the reconstruction error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: original data (Nx784 torch matrix)\n",
    "    reconstructed_data: reconstructed data (Nx784 torch matrix)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    error: float\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" You can use functions in a package.\n",
    "    torch.sum(): https://pytorch.org/docs/stable/generated/torch.sum.html\n",
    "    torch.mean(): https://pytorch.org/docs/stable/generated/torch.sum.html\n",
    "    \"\"\"\n",
    "    \n",
    "    error = torch.mean(torch.sum((data-reconstructed_data)**2), dim = 0) \n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "# (Optional) Use package\n",
    "\"\"\" You can use function in package: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "reconstruction_error = # TODO #\n",
    "\"\"\" \n",
    "reconstruction_error = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epoch = 5000\n",
    "training_data = training_data.to(device)\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    # Forward pass\n",
    "    reconstructed_data = autoencoder(training_data)\n",
    "    loss = reconstruction_error(reconstructed_data, training_data)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"\\r Epoch [{i+1}/{num_epoch}], Loss: {loss.item():.4f}\", end = ' ')\n",
    "\n",
    "last_loss = loss.item()\n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e44b4d0",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Autoencoder-based anomaly detection [3/3]\n",
    "\n",
    "- 테스트 데이터에 대한 **reconstruction error**가 일정 이상으로 크다면, **outlier**로 판단함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450deabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = [] \n",
    "anomaly_score = [] \n",
    "threshhold = 3*last_loss\n",
    "\n",
    "for i in range(0,M):\n",
    "    error = reconstruction_error(test_data[i].view(1,-1),autoencoder(test_data[i].view(1,-1))).item() # Compute the likelihood of test data point\n",
    "    is_anomaly = threshhold < error  # If error is bigger than threshhold, is_anomaly should be True\n",
    "    \n",
    "    anomaly_score.append(error)\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and F1 score\n",
    "accuracy = \"\"\" TODO \"\"\"\n",
    "F1 = \"\"\" TODO \"\"\"\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e44b4d0",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027302e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5x5 subplot\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.title('error: '+str(np.round(anomaly_score[i],3)))\n",
    "    plt.axis('off')\n",
    "print('Normal data samples:')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(test_data[M//2+i].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.title('error: '+str(np.round(anomaly_score[M//2+i],3)))\n",
    "    plt.axis('off')\n",
    "print('Anomaly data samples:')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fda4b5",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "### Pre-trained autoencoder-based anomaly detection [1/3]\n",
    "\n",
    "- 784차원의 데이터를 2차원의 **low dimensional space로 mapping**하는 **pre-trained autoencoder**를 이용함.\n",
    "\n",
    "- 그러면, **low dimensional space**에서 Gaussian model 및 KDE 방법 적용 가능.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "**하지만, pre-trained autoencoder가 사전에 존재해야함.**\n",
    "\n",
    "- 이를 만족 시키기 위해, 임시로 **pre-trained autoencoder**를 좀 더 큰 데이터셋에서 따로 학습시킴.\n",
    "\n",
    "> **training_data_for_pretraining:** pre-trained deep learning model (autoencoder)를 학습하기 위한 데이터셋 ($5000 \\times 2$ torch matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_for_pretraining = torch.cat([data for (data, _) in mnist],dim = 0)[:5000].view(-1,28*28)\n",
    "\n",
    "# Instantiate the autoencoder\n",
    "pre_trained_autoencoder = Autoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(pre_trained_autoencoder.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450deabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-training loop\n",
    "num_epoch = 5000\n",
    "training_data_for_AE = pre_trained_autoencoder.to(device)\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    # Forward pass\n",
    "    reconstructed_data = pre_trained_autoencoder(training_data_for_pretraining)\n",
    "    loss = reconstruction_error(reconstructed_data, training_data_for_pretraining)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"\\r Epoch [{i+1}/{num_epoch}], Loss: {loss.item():.4f}\", end = ' ')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Pre-trained autoencoder-based anomaly detection [2/3]\n",
    "\n",
    "- **Pre-trained autoencoder**를 이용하여, **data**를 **low-dimensional space**에 mapping함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7003e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pre_trained_autoencoder.encoder(training_data.to(device)).cpu().detach().numpy()\n",
    "test_data = pre_trained_autoencoder.encoder(test_data.to(device)).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot training data in green\n",
    "plt.scatter(training_data[:, 0], training_data[:, 1], s = 20,c='green', label='Training Data')\n",
    "# Plot test data\n",
    "plt.scatter(test_data[:, 0], test_data[:, 1], c=[('red' if v else 'blue') for v in test_data_label])\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "\n",
    "### Pre-trained autoencoder-based anomaly detection [3/3]\n",
    "\n",
    "- **Low-dimensional space**에서 **Kernel density estimation**를 사용하여 anomaly detection 수행 (**Gaussian model**을 사용해도 됨)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = []\n",
    "threshhold = 0.05\n",
    "\n",
    "for i in range(0,M):\n",
    "    likelihood = kernel_density_estimation(training_data, test_data[i])\n",
    "    is_anomaly = likelihood < threshhold\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and F1 score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(np.min(test_data[:,0]), np.max(test_data[:,0]), 30), np.linspace(np.min(test_data[:,1]), np.max(test_data[:,1]), 30))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = np.array([kernel_density_estimation(training_data, p) for p in points])\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), \n",
    "                marker='o' if anomaly_predict[i]==test_data_label[i] else 'x')\n",
    "\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "# 3. Anomaly detection with likelihood ratio\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 3.1. Overview\n",
    "\n",
    "**데이터에 대한 likelihood score의 대부분은 background statistics에 큰 영향을 받음.**\n",
    "\n",
    "- 예를 들어 아래와 같은 이미지의 경우, background인 zero pixel이 대부분의 statistics을 차지함.\n",
    "\n",
    "<img src=\"./anomaly_motivation.png\" width=\"600px\" title=\"\" />\n",
    "\n",
    "<br/>\n",
    "\n",
    "- 그러면, 충분한 zero pixel을 포함하는 **anomaly data**와 **normal data**의 **likelihood 차이가 적을 것**.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Likelihood ratio**는 background information을 anomaly detection에서 무시하기 위해 제안된 비교적 최근 프레임 워크 [1].\n",
    "\n",
    "[1] Ren et al., Likelihood Ratios for Out-of-Distribution Detection, NeurIPS 2019\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## 3.2. Method\n",
    "\n",
    "**Likelihood ratio는 다음과 같은 가정을 기반으로함.**\n",
    "\n",
    "- **(Assumption 1)** 각각의 데이터 $x$는 **semantic component**인 $x_S$와 **background componen**t인 $x_B$로 분해될 수 있음 ($p(x)=p(x_B)p(x_S)$).\n",
    "\n",
    "- **(Assumption 2)** 데이터에 **noise perturbation을 추가**해도, **background component**에 대한 **statistics**는 크게 변하지 않음.  \n",
    "\n",
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "**위와 같은 가정에서, likelihood ratio-based anomaly detection은 다음과 같은 프레임워크를 가짐.**\n",
    "\n",
    "- **Ingredient: Noise perturbed data**에 대한 분포를 나타내는 $\\tilde{p}({x})$와 기존의 **training data**에 대한 분포를 나타내는 $p(x)$.\n",
    "\n",
    "- **Likelihood ratio**은 주어진 data x'에 대하여, 두 분포에 대한 **likelihood ratio**를 다음과 같이 정의함.\n",
    "\n",
    "$$\\text{LLR}(x')=\\frac{p(x')}{\\tilde{p}(x')}=\\frac{p({x'}_B)p({x'}_S)}{\\tilde{p}({x'}_B)\\tilde{p}({x'}_S)}\\approx\\frac{p({x'}_S)}{\\tilde{p}({x'}_S)},\\qquad \\text{(Assumption 2) } \\tilde{p}({x'}_B)\\approx p({x'}_B)$$\n",
    "\n",
    "- 즉,  $\\text{LLR}(x')$는 **semantic component**만을 기반으로 **likelihood score**를 제공함.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 3.3. Implementation\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Define perturbed data\n",
    "\n",
    "- 우선, pertured data distribution $\\tilde{p}(x)$를 학습하기 위한 **perturbed dataset** $\\{\\tilde{x}_1,\\ldots,\\tilde{x}_N\\}$을 정의함. 이는 단순히 training data에 **random noise**를 추가하여 얻을 수 있음.\n",
    "\n",
    "> **perturbed_training_data**: Noise perturbation이 추가된 perturebed dataset ($N \\times 2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f54ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" You can sample noise from various random distribution.\n",
    "normal distribution: https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html\n",
    "uniform distribution: https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html\n",
    "(recomendation) np.random.normal(# TODO #)\n",
    "\"\"\" \n",
    "\n",
    "perturbed_training_data =  training_data + np.random.normal(0,5, training_data.shape) # add perturbation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6f0d0a",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "### Likelihood ratio-based Anomaly detection with Gaussian Model [1/2]\n",
    "\n",
    "- Perturbed data distribution인 $\\tilde{p}(x)$와, normal training data distribution인 $p(x)$를 각각 학습해야함.\n",
    "\n",
    "- Gaussian model을 이용하여 각각의 data distribution을 표현함. $$p(x)=p(x;\\mu,\\Sigma),\\qquad \\tilde{p}(x)=p(x;\\tilde{\\mu},\\tilde{\\Sigma})$$ $\\mu,\\Sigma$는 normal training data $\\{x_1,\\ldots, x_N\\}$에 대한 분포를 표현하며, $\\tilde{\\mu},\\tilde{\\Sigma}$는 perturbed data $\\{\\tilde{x}_1,\\ldots, \\tilde{x}_N\\}$에 대한 분포를 표현함. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" You can use fit_Gaussian_parameter() function.\n",
    "mean, cov: mean and covariance parameters of normal training data distribution.\n",
    "perturbed_mean, perturbed_cov: mean and covariance parameters of perturbed training data distribution.\n",
    "\"\"\" \n",
    "\n",
    "# The parameters of Gaussian model trained on training data\n",
    "mean, cov = fit_Gaussian_parameter(training_data)\n",
    "\n",
    "# The parameters of Gaussian model trained on perturbed data\n",
    "perturbed_mean, perturbed_cov = fit_Gaussian_parameter(perturbed_training_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b677bdcc",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Likelihood ratio-based Anomaly detection with Gaussian Model [2/2]\n",
    "\n",
    "- 각각 학습된 모델을 기반으로, $\\text{LLR}(x')={p(x')}/{\\tilde{p}(x')}$를 계산할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predict = []\n",
    "threshhold = 1.2\n",
    "\n",
    "for i in range(M):\n",
    "\n",
    "    # Compute LLR score\n",
    "    p_x = compute_likelihood_Gaussian(test_data[i], mean, cov)\n",
    "    p_perturbed_x = compute_likelihood_Gaussian(test_data[i], perturbed_mean, perturbed_cov)\n",
    "    LLR_x = p_x/p_perturbed_x\n",
    "\n",
    "    # Is anomaly?\n",
    "    is_anomaly = LLR_x < threshhold\n",
    "    anomaly_predict.append(is_anomaly)\n",
    "\n",
    "print('Prediction: ', anomaly_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4612d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and F1 score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_data_label, anomaly_predict)\n",
    "F1 = sklearn.metrics.f1_score(test_data_label, anomaly_predict)\n",
    "print('Accuracy: ', accuracy)\n",
    "print('F1 score: ', F1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc08191",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Visualize results\n",
    "\n",
    "- **'o'** indicates the prediciton is correct, whereas **'x'** indicates the prediction is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary\n",
    "x, y = np.meshgrid(np.linspace(np.min(test_data[:,0]), np.max(test_data[:,0]), 30), np.linspace(np.min(test_data[:,1]), np.max(test_data[:,1]), 30))\n",
    "points = np.column_stack((x.ravel(), y.ravel()))\n",
    "pdf_values = np.array([compute_likelihood_Gaussian(p, mean, cov)/compute_likelihood_Gaussian(p, perturbed_mean, perturbed_cov)\n",
    "                       for p in points])\n",
    "\n",
    "# Plot the contour of the PDF\n",
    "plt.contour(x, y, pdf_values.reshape(x.shape), levels=[threshhold], colors='green', linewidths=2, linestyles='dashed')\n",
    "\n",
    "# Plot test data\n",
    "markers = np.where(anomaly_predict==test_data_label, \"o\", \"x\").tolist()\n",
    "for i in range(M):\n",
    "    plt.scatter(test_data[i, 0], test_data[i, 1], c=('red' if test_data_label[i] else 'blue'), \n",
    "                marker='o' if anomaly_predict[i]==test_data_label[i] else 'x')\n",
    "\n",
    "\n",
    "plt.title('Data Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
