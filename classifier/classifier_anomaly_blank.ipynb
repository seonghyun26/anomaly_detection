{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview - Classifier based anomaly detection \n",
    "\n",
    "Deep neural networks (DNNs)은 test sample들이 similar distribution(i.e., in-distribution)에 있을 경우에는 잘 작동합니다. (ex. Image classifier)\n",
    "\n",
    "- 하지만 만약 sample이 out-of-distribution로부터 왔다면 DNNs은 제대로 작동하지 못합니다.\n",
    "\n",
    "- 따라서 test sample이 in-distribution인지, out-of-distribution인지 탐지해야 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How?\n",
    "\n",
    "<img src=\"./images/Classifier_overview.png\" width=\"1000px\" title=\"Classifier Overview\" />\n",
    "\n",
    "\n",
    "Threshold-based classification, 즉 임계값을 바탕으로 분류해 이를 수행합니다.\n",
    "\n",
    "여기서는 classifier를 이용해 self-supervised에 초점을 두고 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Installing library & datasets\n",
    "\n",
    "터미널에서 아래 명령어를 차례대로 입력해 conda 환경을 설정합니다.\n",
    "\n",
    "- wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh\n",
    "\n",
    "- bash Anaconda3-2021.11-Linux-x86_64.sh\n",
    "\n",
    "Open new terminal (Top menu > Terminal > New Terminal), create conda environment\n",
    "\n",
    "- conda create -n anomaly_detection python=3.6.9\n",
    "\n",
    "Connect to conda environment\n",
    "\n",
    "- Select Kernel > Python environments > anomaly_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Library\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install numpy\n",
    "!pip3 install tqdm\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and save MNIST dataset\n",
    "mnist_train = torchvision.datasets.MNIST(root='./datasets', train=True, transform=transform, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./datasets', train=False, transform=transform, download=True)\n",
    "\n",
    "# Download and save CIFAR-10 dataset\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='./datasets', train=True, transform=transform, download=True)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='./datasets', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Supervsied learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1.1. Classifier preliminaries\n",
    "\n",
    "---\n",
    "\n",
    "- Classification은 unknown posterior distribution, i.e., P(Y|X)를 찾는 것이었습니다. \n",
    "\n",
    "- 이때 posterior distribution은 DNN을 이용한 softmax classifier로 다음과 같이 쓸 수 있습니다.\n",
    "\n",
    "$$ P(y=c|\\mathbf{x}) = \\frac{\\exp ( \\bold{w}_{c}^{T}f(x)+b_{c})}{ \\sum_{c'}\\exp (\\bold{w}_{c'}^{T}f(x)+b_{c'})}$$\n",
    "\n",
    "- 먼저 간단한 classifier를 정의하고 학습합니다.\n",
    "  \n",
    "- 학습한 classifier를 통해 confidence score를 계산합니다.\n",
    "  \n",
    "- Evaluation metric을 통해 confidenc score의 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Classifier\n",
    "\n",
    "우선 MNIST dataset에 대해 간단한 classifier를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classifier, self).__init__()\n",
    "    \"\"\"\n",
    "    TODO: Define linear layers\n",
    "    Input image shape: (28, 28)\n",
    "    Hidden layer dimension: 16\n",
    "    Final Output: 2 classes, 0 as anomaly(out-of-distribution) and other digit 0 ~ 9 as in-distribution\n",
    "    HINT: nn.Linear(), https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    \"\"\"\n",
    "    self.fc1 = \n",
    "    self.fc2 = \n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    TODO: Define the forward functions\n",
    "    1. Flatten the image (HINT: torch.view(), https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)\n",
    "    2. Apply activation function torch.relu() to the results of fc1 and fc2\n",
    "    \"\"\"\n",
    "    x = \n",
    "    hidden = \n",
    "    out = \n",
    "    return out, hidden\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 MNIST Dataset\n",
    "- MNIST는 숫자 0~9의 이미지에 관한 데이터셋입니다.\n",
    " \n",
    "- 여기서 0을 anomaly, i.e., out-of-distribution data로 생각해 label 1로,\n",
    "  나머지 숫자 1~9를 in-distrubution으로 생각해 label 0으로 분류합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Set labels for anomaly detection\n",
    "mnist_train.targets = torch.where(mnist_train.targets == 0, torch.tensor(1), torch.tensor(0))\n",
    "mnist_test.targets = torch.where(mnist_test.targets == 0, torch.tensor(1), torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Visualize normal & anomaly datas\n",
    "mnist_test_normal_indices = (mnist_test.targets != 1).nonzero().squeeze()\n",
    "mnist_test_anomaly_indices = (mnist_test.targets == 1).nonzero().squeeze()\n",
    "\n",
    "number_of_samples = 25\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "for i, idx in enumerate(mnist_test_normal_indices[:number_of_samples]):\n",
    "    plt.subplot(1, number_of_samples, i + 1)\n",
    "    plt.imshow(mnist_test[idx][0].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "print('Normal data samples:')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "for i, idx in enumerate(mnist_test_anomaly_indices[:number_of_samples]):\n",
    "    plt.subplot(1, number_of_samples, i + 1)\n",
    "    plt.imshow(mnist_test[idx][0].numpy().reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "print('Anomaly data samples:')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Training a simple classifier\n",
    "\n",
    "- 위에서 정의한 classifier를 훈련합니다.\n",
    "- 훈련한 model을 저장해 나중에 사용할 수 있도록 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=False)\n",
    "min_gram_value = float('inf')\n",
    "max_gram_value = float('-inf')\n",
    "\n",
    "# NOTE: Initialize the classifier and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier = Classifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# NOTE: Training loop\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # NOTE: calculated gram matrix value        \n",
    "        # if epoch == 9:\n",
    "        #     with torch.no_grad():\n",
    "        #         _, hidden_features = classifier(images)\n",
    "        #         for idx, feature in enumerate(hidden_features):\n",
    "        #             if labels[idx].item() == 0:\n",
    "        #                 feature = feature.unsqueeze(0)\n",
    "        #                 gram_matrix = torch.matmul(feature, feature.t())\n",
    "        #                 min_gram_value = min(min_gram_value, gram_matrix.min().item())\n",
    "        #                 max_gram_value = max(max_gram_value, gram_matrix.max().item())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# NOTE: Save the trained model\n",
    "torch.save(classifier.state_dict(), 'classifier_trained.pth')\n",
    "print(\"Trained model saved as 'classifier_trained.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier = Classifier().to(device)\n",
    "\n",
    "# NOTE: 직접 학습한 모델을 불러오는 경우\n",
    "# classifier.load_state_dict(torch.load('classifier_trained.pth'))\n",
    "# NOTE: pretrained model을 불러오는 경우\n",
    "classifier.load_state_dict(torch.load('classifier_pretrained.pth'))\n",
    "min_gram_value = 70\n",
    "max_gram_value = 1500\n",
    "\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Confidence score\n",
    "\n",
    "이제 classifier로 구한 posterior distribution에서 아래의 두 가지 방법으로 confidence score를 구할 수 있습니다.\n",
    "1. Posterior distribution 값들 중 최댓값을 사용합니다\n",
    "$$ \\max_{c} \\ P(y=c|\\mathbf{x}) $$\n",
    "2. Posterior distribution의 entropy를 활용합니다\n",
    "$$ H = \\sum_{y}-P(y|\\mathbf{x}) \\log P(y|\\mathbf{x}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_scores(outputs):\n",
    "  posterior_probs = outputs\n",
    "  \"\"\"\n",
    "  TODO: posterior_probs를 이용하여 max_prob과 entropy를 계산하세요.\n",
    "  1. Maximum value in posterior distribution\n",
    "  HINT: torch.max() 함수를 사용하세요, https://pytorch.org/docs/stable/generated/torch.max.html#torch-max\n",
    "  \"\"\"\n",
    "  max_prob_value, max_prob_idx = \n",
    "  \n",
    "  \"\"\"\n",
    "  2. Entropy of the posterior distribution\n",
    "  HINT: torch.log2() 함수를 사용하세요, https://pytorch.org/docs/stable/generated/torch.log2.html\n",
    "  로그 함수는 양수만을 입력해야 되므로, 혹시 모를 경우를 대비해 임의의 작은 값을 더해줘도 됩니다.\n",
    "  \"\"\"\n",
    "  entropy = \n",
    "  \n",
    "  return max_prob_idx, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Visualization\n",
    "\n",
    "앞에서 구현한 두 가지 confidenc score로 anomaly detection 시각화합니다.\n",
    "\n",
    "- test sample 20개에 대한 confidence score를 확인합니다.\n",
    "\n",
    "- 현재 상황에서 0을 anomaly, i.e., out-of-distribution, 1 ~ 9 를 in-distribution으로 생각합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the anomaly detection classifier\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=64, shuffle=False)\n",
    "\n",
    "def visualization(test_loader):\n",
    "    threshold = 0.8400\n",
    "\n",
    "    num_anomalies_to_visualize = 30\n",
    "    num_cols = 6\n",
    "    num_rows = 5\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            output, _ = classifier(images.to(device))\n",
    "            output = torch.softmax(output, dim=1)\n",
    "            max_prob_idx, entropy = calculate_confidence_scores(output)\n",
    "            print(output)\n",
    "            \n",
    "            for i, image in enumerate(images):\n",
    "                if i >= num_anomalies_to_visualize:\n",
    "                    break\n",
    "                max_prob_idx, entropy = calculate_confidence_scores(output)\n",
    "                plt.subplot(num_rows, num_cols, i + 1)\n",
    "                plt.imshow(image.squeeze(), cmap='gray')\n",
    "                plt.title(f'prob: {output[i][0].item():.2f}, {output[i][1].item():.2f}, \\nMax Prob: {max_prob_idx[i].item()},\\n Entropy: {entropy[i].item():.4f},\\n Label: {labels[i].item()}')\n",
    "                plt.axis('off')\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualization(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 Evaluation metrics\n",
    "\n",
    "Out-of-distribution을 얼마자 잘 하였는지 평가하기 위해 사용하는 지표 AUPR(Area under the Precision-Recall curve), AUROC(Area under ROC curve)가 있었습니다.\n",
    "\n",
    "<img src=\"./images/Confusion_matrix.png\" width=\"452px\" title=\"Confusion_matrix\" />\n",
    "<img src=\"./images/AUPR_AUROC.png\" width=\"1000px\" title=\"AUPR_AUROC\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auroc_aupr(true_labels, predicted_probs):\n",
    "    sorted_indices = np.argsort(predicted_probs)[::-1]\n",
    "    true_labels_sorted = true_labels[sorted_indices]\n",
    "    num_anomalies = np.sum(true_labels_sorted == 1)\n",
    "    num_normals = len(true_labels_sorted) - num_anomalies\n",
    "    \"\"\"\n",
    "    TODO: AUROC, AUPR 계산\n",
    "    - true_labels_sorted: 올바르게 분류한 데이터들\n",
    "    - num_anomalies: TP + FN\n",
    "    - num_normals: FP + TN\n",
    "    아래 값들을 바탕으로 true_positive, false_positive을 계산합니다.\n",
    "    이후 AUROC를 계산하기 위한 tpr, fpr, 그리고 AUPR을 계산하기 위한 precision, recall을 계산합니다.\n",
    "    HINT: np.cumsum(), https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html\n",
    "    \"\"\"\n",
    "    true_positive = \n",
    "    false_positive = \n",
    "\n",
    "    tpr = \n",
    "    fpr = \n",
    "    auroc = np.trapz(tpr, fpr)\n",
    "\n",
    "    precision = \n",
    "    recall = \n",
    "    aupr = np.trapz(precision, recall)\n",
    "\n",
    "    return auroc, aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    outputs, _ = classifier(images)\n",
    "    outputs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    predicted_probs.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()) \n",
    "    true_labels.extend(labels.numpy())\n",
    "\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_probs = np.array(predicted_probs)\n",
    "auroc, aupr = calculate_auroc_aupr(true_labels, predicted_probs)\n",
    "print(f'Caculated AUROC: {auroc:.4f}')\n",
    "print(f'Caculated AUPR: {aupr:.4f}\\n')\n",
    "\n",
    "func_auroc = roc_auc_score(true_labels, predicted_probs)\n",
    "func_aupr = average_precision_score(true_labels, predicted_probs)\n",
    "print(f'Library AUROC: {func_auroc:.4f}')\n",
    "print(f'Library AUPR: {func_aupr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Calibrating Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Neural network의 경우 out-of-distribution data에 대해 overconfident한 경향이 있습니다.\n",
    "\n",
    "- 예측한 class label 확률이 참값의 truth correctness likelihodd를 반영해야 된다는 것입니다.\n",
    "\n",
    "- 이를 위해 ODIN detector, post-processing을 이용해 posterior distribution 보정합니다.\n",
    "\n",
    "- Posterior distribution을 보정하는 작업이기에, test 때 이루어집니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Temperature scaling\n",
    "\n",
    "posterior distribution을 완화하여 overconfidence을 극복하는 방법입니다.\n",
    "\n",
    "$$ P(y=\\hat{y}|\\mathsf{x};T) = \\frac{ \\exp (f_{\\hat{y}}(\\mathsf{x})/T) }{\\sum_y \\exp (f_{y}(\\mathsf{x})/T)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_scaling(logits, temperature):\n",
    "  # TODO; temperature scaling 구현\n",
    "  scaled_logits = \n",
    "  \n",
    "  scaled_probs = torch.softmax(scaled_logits, dim=1)\n",
    "  \n",
    "  return scaled_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Input processing\n",
    "\n",
    "- 유사하게, noise를 추가해 posterior distribution을 완화하여 overconfidence을 극복하는 방법입니다\n",
    "\n",
    "$$ x' = x - \\epsilon * sign (-\\nabla_x \\log P(y=\\hat(y)|x;T)) $$\n",
    "\n",
    "Modified confidence score: \n",
    "\n",
    "$$ \\max_y P(y|\\mathsf{x}';T) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_processing(model, images, labels, epsilon):\n",
    "  model.eval()\n",
    "  images.required_grad = True\n",
    "  \n",
    "  outputs, _ = model(images)\n",
    "  loss = criterion(outputs, labels)\n",
    "  loss.backward()\n",
    "  \"\"\"\n",
    "  TODO: inptut processing을 구현합니다.\n",
    "  HINT: .grad.sign()을 이용합니다, https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html\n",
    "  \"\"\"\n",
    "  perturbed_outputs = \n",
    "  \n",
    "  return perturbed_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Gather up\n",
    "\n",
    "앞의 두 가지 방법을 이용하여 다시 test sample에 대해 실허해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: hyper parameters\n",
    "temperature = 100.0\n",
    "epsilon = 0.001 \n",
    "\n",
    "predicted_probs_calibrated = []\n",
    "true_labels2 = []\n",
    "\n",
    "classifier.eval()\n",
    "classifier = classifier.to('cpu')\n",
    "for images, labels in test_loader:\n",
    "    images.requires_grad = True\n",
    "    outputs, _ = classifier(images)\n",
    "    \n",
    "    # NOTE: input preprocessing\n",
    "    perturbed_images = input_processing(classifier, images, labels, epsilon)\n",
    "    outputs, _ = classifier(perturbed_images)\n",
    "    \n",
    "    # NOTE: Temperature scaling\n",
    "    scaled_outputs = temperature_scaling(outputs, temperature)\n",
    "    \n",
    "    predicted_probs_calibrated.extend(torch.softmax(scaled_outputs, dim=1)[:, 1].cpu().detach().numpy()) \n",
    "    true_labels2.extend(labels.numpy())\n",
    "\n",
    "print(f'Original AUROC: {func_auroc:.6f}')\n",
    "print(f'Original AUPR: {func_aupr:.6f}')\n",
    "calibrated_auroc = roc_auc_score(true_labels2, predicted_probs_calibrated)\n",
    "calibrated_aupr = average_precision_score(true_labels2, predicted_probs_calibrated)\n",
    "print(f'Calibrated AUROC: {calibrated_auroc:.6f}')\n",
    "print(f'Calibrated AUPR: {calibrated_aupr:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Using hidden layer features\n",
    "\n",
    "---\n",
    "\n",
    "- DNN의 hidden feature 중에는 training data 중 의미 있는 feature들이 존재합니다.\n",
    "\n",
    "- 이를 활용해 abnormal samples을 찾아냅니다.\n",
    "\n",
    "<img src=\"./images/Hidden_features.png\" width=\"1000px\" title=\"Hidden features\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Gram matrix\n",
    "\n",
    "<img src=\"./images/Gram_matrix.png\" width=\"600px\" title=\"Gram_matrix\" />\n",
    "\n",
    "\n",
    "- Gram matrices을 사용해 hidden feature의 correlation을 구합니다.\n",
    "\n",
    "- Test sample 중 training data와 다른 gram matrix value를 가진 sample을 찾습니다.\n",
    "\n",
    "- 위에서 정의한 간단한 classifier의 경우 hidden layer가 하나이므로, fc1의 통과한 hidden feature를 이용합니다.\n",
    "\n",
    "- 앞서 training 때 normal data에 대해 구한 gram matrix value은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_gram_value)\n",
    "print(max_gram_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Distance function\n",
    "- 아래 사진과 같이 gram matrix value에 대한 거리값을 구합니다.\n",
    "\n",
    "<img src=\"./images/Gram_matrix_distance_function.png\" width=\"600px\" title=\"Gram matrix distance function\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gram_distance(gram_matrix, min_gram_value, max_gram_value):\n",
    "  \"\"\"\n",
    "  TODO: 위 사진과 같이 min_gram_value와 max_gram_value 사이에 있는지 확인하고,\n",
    "  그 사이에 있으면 0을, 그렇지 않으면 상대적 거리를 반환하는 함수를 구현하세요.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_indices = []\n",
    "gram_matrix_list = []\n",
    "gram_distance_threshold = 0.22\n",
    "\n",
    "min_gram_value = 70\n",
    "max_gram_value = 1500\n",
    "\n",
    "# Calculate Gram matrix-based scores\n",
    "with torch.no_grad():\n",
    "  for idx, (images, labels) in enumerate(test_loader):\n",
    "    _, hidden_features = classifier(images)\n",
    "    for idx2, feature in enumerate(hidden_features):\n",
    "      feature = feature.unsqueeze(0)\n",
    "      gram_matrix = torch.matmul(feature, feature.t())\n",
    "      gram_distance = calculate_gram_distance(gram_matrix, min_gram_value, max_gram_value)\n",
    "      \n",
    "      if gram_distance > gram_distance_threshold:\n",
    "        print(gram_matrix, gram_distance)\n",
    "        ood_indices.append(idx * 64 + idx2)\n",
    "        gram_matrix_list.append(gram_distance.item())\n",
    "    \n",
    "# Print the detected OOD sample indices\n",
    "print(\"Detected OOD Sample \", len(ood_indices))\n",
    "print(\"Detected OOD Sample Indices:\", ood_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Visualization\n",
    "\n",
    "- gram matrix value들로 찾은 anomaly data를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "num_anomalies_to_visualize = 36\n",
    "num_cols = 6\n",
    "num_rows = num_anomalies_to_visualize // num_cols + 1\n",
    "\n",
    "with torch.no_grad():\n",
    "  for idx, oodIdx in enumerate(ood_indices[:num_anomalies_to_visualize]):\n",
    "    image, label = mnist_test[oodIdx]\n",
    "    plt.subplot(num_rows, num_cols, idx + 1)\n",
    "    \n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.title(f'Gram matrix distance: {gram_matrix_list[idx]:.2f}\\nLabel: {label}\\\n",
    "    ')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Github - awesome-anomaly-detection](https://github.com/hoya012/awesome-anomaly-detection#out-of-distributionood-detection-target)\n",
    "\n",
    "[Calibrating Neural networks using temperature scaling & input processing](https://arxiv.org/pdf/1706.02690.pdf)\n",
    "\n",
    "[Detecting OOD examples using Gram Matrices](https://arxiv.org/pdf/1912.12510.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
